{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural translation model\n",
    "This model attempts to use an autoencoder for a translation task. We will be using the language dataset from http://www.manythings.org/anki/ to build a neural translation model. This dataset consists of over 200,000 pairs of sentences in English and German. In order to make the training quicker, we will restrict to our dataset to 20,000 pairs. Feel free to change this if you wish - the size of the dataset used is not part of the grading rubric.\n",
    "\n",
    "\n",
    "We'll start by running some imports, and loading the dataset. For this project you are free to make further imports throughout the notebook as you wish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the dataset\n",
    "NUM_EXAMPLES = 20000\n",
    "data_examples = []\n",
    "with open('data/deu.txt', 'r', encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "        if len(data_examples) < NUM_EXAMPLES:\n",
    "            data_examples.append(line)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions preprocess English and German sentences\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"ü\", 'ue', sentence)\n",
    "    sentence = re.sub(r\"ä\", 'ae', sentence)\n",
    "    sentence = re.sub(r\"ö\", 'oe', sentence)\n",
    "    sentence = re.sub(r'ß', 'ss', sentence)\n",
    "    \n",
    "    sentence = unicode_to_ascii(sentence)\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r\"[^a-z?.!,']+\", \" \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_examples[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =[]\n",
    "for i in data_examples:\n",
    "    sentences.append(i.split(\"\t\"))\n",
    "\n",
    "sentences[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences=[]\n",
    "german_sentences =[]\n",
    "for s in sentences:\n",
    "    english_sentences.append(s[0])\n",
    "    german_sentences.append(s[1])\n",
    "   \n",
    "\n",
    "index=10000\n",
    "\n",
    "print(english_sentences[index], german_sentences[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = '<start>'\n",
    "end_token = '<end>'\n",
    "\n",
    "for es in english_sentences:\n",
    "    es=preprocess_sentence(es)\n",
    "    \n",
    "for gs in german_sentences:\n",
    "    gs=start_token+preprocess_sentence(gs)+end_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(german_sentences[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(filters='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tokenizer.fit_on_texts(german_sentences)\n",
    "config = tokenizer.get_config()\n",
    "word_index = json.loads(config['word_index'])\n",
    "index_words  = json.loads(config['index_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples =5\n",
    "inx = np.random.choice(len(english_sentences),num_samples, replace=False)\n",
    "print(inx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "sequences=tokenizer.texts_to_sequences(german_sentences)\n",
    "padded = pad_sequences(sequences, padding='post',value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data with tf.data.Dataset objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hub module: https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1.\n",
    "\n",
    "This embedding takes a batch of text tokens in a 1-D tensor of strings as input. It then embeds the separate tokens into a 128-dimensional space. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer = tf.keras.models.load_model('./models/tf2-preview_nnlm-en-dim128_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the layer\n",
    "emb_layer(tf.constant([\"these\", \"aren't\", \"the\", \"droids\", \"you're\", \"looking\", \"for\"])).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(german_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_english_train, x_english_test = train_test_split(english_sentences, test_size=0.2)\n",
    "x_german_train, x_german_test = train_test_split(sequences, test_size=0.2)\n",
    "\n",
    "\n",
    "x_english_train = np.array(x_english_train)\n",
    "x_english_test = np.array(x_english_test)\n",
    "x_german_train = np.array(x_german_train)\n",
    "x_german_test = np.array(x_german_test)\n",
    "\n",
    "trainDataset = tf.data.Dataset.from_tensor_slices((x_english_train,x_german_train))\n",
    "validationDataset = tf.data.Dataset.from_tensor_slices((x_english_test,x_german_test ))\n",
    "\n",
    "\n",
    "print(trainDataset.element_spec)\n",
    "print(validationDataset.element_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x,y):\n",
    "    inp =[]\n",
    "    inp.append(tf.strings.split(x,''))\n",
    "    inp.append(y)\n",
    "    return inp[0],inp[1]\n",
    "\n",
    "trainDataset=trainDataset.map(split)\n",
    "validationDataset=validationDataset.map(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_f(x,y):\n",
    "    inp=[]\n",
    "    inp.append(emb_layer(x))\n",
    "    inp.append(y)\n",
    "    return inp[0], inp[1]\n",
    "\n",
    "trainDataset=trainDataset.map(embedding_f)\n",
    "validationDataset=validationDataset.map(embedding_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset.element_spec\n",
    "validationDataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = trainDataset.filter(lambda x,y: False if ((tf.shape(x)[0])>13 else True))\n",
    "validationDataset = validationDataset.filter(lambda x,y: False if ((tf.shape(x)[0])>13 else True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_embedding_f(x,y):\n",
    "    inp=[]\n",
    "    pad=tf.pad(x, paddings=[[13-tf.shape(x)[0],0][0,0]], mode='CONSTANT')\n",
    "    inp.append(pad)\n",
    "    inp.append(y)\n",
    "    return inp[0], inp[1]\n",
    "\n",
    "trainDataset=trainDataset.batch(16)\n",
    "validationDataset=validationDataset.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset=trainDataset.batch(map_embedding_f)\n",
    "validationDataset=validationDataset.map(map_embedding_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create the custom layer\n",
    "You will now create a custom layer to add the learned end token embedding to the encoder model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "class CustomLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "        self.learned_emb = tf.Variable(initial_value=tf.zeros(shape=(1,128),dtype=tf.float32))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x=self.learned_emb\n",
    "        x=tf.tile(x,[tf.shape(inputs)[0],1])\n",
    "        x=tf.expand_dims(x,axis=1, name=None)\n",
    "        return tf.keras.layers.Concatenate(axis=1)([inputs,x])\n",
    "    \n",
    "myLayer = CustomLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Masking, LSTM\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "inputs = Input(batch_shape = (None, 13, 128))\n",
    "x = myLayer(inputs)\n",
    "x = Masking(mask_value = 4.0)(x)\n",
    "hidden_state, cell_state = LSTM(512, return_state=True, name='stateful')(x)\n",
    "encoder = Model(inputs = inputs, outputs = [hidden_state, cell_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the decoder network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,Dropout\n",
    "\n",
    "\n",
    "class MyDecoder(Model):\n",
    "    def __init__(self):\n",
    "        super(MyDecoder,self).__init__()\n",
    "        self.embedding=Embedding(len(tokenizer.word_index),128,mask_zero=True)\n",
    "        self.lstm=LSTM(512,return_sequences=True, return_state=True)\n",
    "        self.dense=Dense(len(tokenizer.word_index))\n",
    "    def call(self,inputs, hidden_state=None,cell_state=None):\n",
    "        x=self.embedding(inputs)\n",
    "        if hidden_state is not None and cell_state is not None:\n",
    "            x, hidden_state, cell_state=self.lstm(x, initial_state=[hidden_state, cell_state])\n",
    "        else:\n",
    "            x, hidden_state, cell_state=self.lstm(x)\n",
    "            x=self.dense(x)\n",
    "        return x, hidden_state, cell_state\n",
    "        \n",
    "        decoder=MyDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden, cell = Encoder(next(iter(trainDataset.take(1)))[0])\n",
    "print(decoder(next(iter(trainDataset.take(1))))[1],hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "    german_inputs = tf.zeros([data.shape[0],data.shape[1], tf.int32])\n",
    "    german_outputs = tf.zeros([data.shape[0],data.shape[1], tf.int32])\n",
    "    german_inputs=data\n",
    "    german_inputs = tf.where(german_inputs[:]==word_index.get(end_token),x=0,y=german_inputs)\n",
    "    german_outputs = tf.where(german_inputs[:]==word_index.get(start_token),x=0,y=german_inputs)\n",
    "\n",
    "    return german_inputs, german_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(0.05)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossEntropy(from_logits=True)\n",
    "variables = Encoder.trainable_variables+ Decoder.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def grad_fn(data1, data, german_outputs, loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x,y = Encoder(data1)\n",
    "        a,b,c = Decoder(data, x, y)\n",
    "        loss_value=loss(german_outputs, loss)\n",
    "    return loss_value, tape.gradient(loss_value, variables)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(num_epochs, dataset, val_dataset, val_steps, grad_fn, optimizer, loss):\n",
    "    train_loss_results=[]\n",
    "    val_loss_results = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss_avg=tf.keras.metrics.Mean()\n",
    "        val_epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        \n",
    "        for x,y in dataset:\n",
    "            inp, out = convert(y)\n",
    "            loss_value, grads = grad_fn(x,y,out, loss)\n",
    "            optimizer.apply_gradients(zip(grads, variables))\n",
    "            \n",
    "            epoch_loss_avg(loss_value)\n",
    "        for a,b in val_dataset:\n",
    "            preds,_,_ = decoder(b)\n",
    "            val_epoch_loss_avg(loss(out, preds))\n",
    "        train_loss_results.append(epoch_loss_avg.result())\n",
    "        val_loss_results.append(val_epoch_loss_avg.result())\n",
    "        \n",
    "        print(\" Epoch :{03d}: Loss: {:.3f} : Val_loss: {:.3f}\".format(epoch, epoch_loss_results(), val_epoch_loss_results()))\n",
    "        \n",
    "    return train_loss_results, val_loss_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
